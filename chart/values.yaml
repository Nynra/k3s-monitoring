global:
  commonLabels: {}
  commonAnnotations: {}

enabled: true

# This chart does not use the ingress provided by the kube-prometheus-stack chart.
# Instead the ingress and certs are provided in the templates of this chart.
grafanaIngress:
  enabled: true
  ingressUrl: "grafana.example.com"

  certName: "grafana-example-com-tls"
  externalCert:
    enabled: true
    secretName: "grafana-example-com-tls"
    secretStore: "kubernetes"
    secretStoreType: "ClusterSecretStore"

kube-prometheus-stack:
  enabled: true
  grafana:
    # Grafana is a dependancy of the kube-prometheus-stack chart, so we can configure it here
    # and all values will be directed towards the grafana chart.
    enabled: true
    defaultDashboardsEnabled: true
    defaultDashboardsTimezone: utc
    defaultDashboardsInterval: 1m

    ingress: 
      enabled: false
    
    # # To make Grafana persistent (Using Statefulset)
    # persistence:
    #   enabled: true
    #   type: sts
    #   storageClassName: "storageClassName"
    #   accessModes:
    #     - ReadWriteOnce
    #   size: 20Gi
    #   finalizers:
    #     - kubernetes.io/pvc-protection

    # Grafana Operator is End-of-Life and will be removed in future releases.
    operator: 
      ## Enable references to ConfigMaps containing dashboards in GrafanaDashboard CRs
      ## Set to true to allow dashboards to be loaded from ConfigMap references
      dashboardsConfigMapRefEnabled: false

      # ## Labels that should be matched kind: Grafana instance
      # matchLabels: { category: dashboard }

      # ## Which folder all dashboards in Grafana General means on Root level
      # folder: "default"

      # ## How frequently the operator should resync resources (in duration format)
      # ## Controls how often dashboards are reconciled by the operator
      # ##
      # resyncPeriod: 10m
    
    sidecar:
      dashboards: 
        enabled: true 
        label: grafana_dashboard
        labelValue: "true"
        searchNamespace: "all"

        # Support for new table panels, when enabled grafana auto migrates the old table panels to newer table panels
        enableNewTablePanelSyntax: false

        provider:
          allowUiUpdates: false

      datasources: 
        enabled: true 
        defaultDatasourceEnabled: true
        isDefaultDatasource: true

        name: Prometheus
        uid: prometheus

        # If not defined, will use prometheus.prometheusSpec.scrapeInterval or its default
        # defaultDatasourceScrapeInterval: 15s

        alertmanager:
          enabled: true
          name: Alertmanager
          uid: alertmanager
          handleGrafanaManagedAlerts: false
          implementation: prometheus

    additionalDataSources: []
      # - name: prometheus-sample
      #   access: proxy
      #   basicAuth: true
      #   secureJsonData:
      #       basicAuthPassword: pass
      #   basicAuthUser: daco
      #   editable: false
      #   jsonData:
      #       tlsSkipVerify: true
      #   orgId: 1
      #   type: prometheus
      #   url: https://{{ printf "%s-prometheus.svc" .Release.Name }}:9090
      #   version: 1

    # Flag to mark provisioned data sources for deletion if they are no longer configured.
    # It takes no effect if data sources are already listed in the deleteDatasources section.
    # ref: https://grafana.com/docs/grafana/latest/administration/provisioning/#example-data-source-configuration-file
    prune: false
  
  prometheus: 
    prometheusSpec:
      ## Statefulset's persistent volume claim retention policy
      ## whenDeleted and whenScaled determine whether
      ## statefulset's PVCs are deleted (true) or retained (false)
      ## on scaling down and deleting statefulset, respectively.
      ## Requires Kubernetes version 1.27.0+.
      ## Ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#persistentvolumeclaim-retention
      persistentVolumeClaimRetentionPolicy: {}

      ## Interval between consecutive scrapes.
      ## Defaults to 30s.
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/release-0.44/pkg/prometheus/promcfg.go#L180-L183
      scrapeInterval: "30s"

      ## Number of seconds to wait for target to respond before erroring
      scrapeTimeout: ""

      ## EnableAdminAPI enables Prometheus the administrative HTTP API which includes functionality such as deleting time series.
      ## This is disabled by default.
      ## ref: https://prometheus.io/docs/prometheus/latest/querying/api/#tsdb-admin-apis
      enableAdminAPI: false

      ## Prometheus StorageSpec for persistent data
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/platform/storage.md
      storageSpec: {}
      ## Using PersistentVolumeClaim
      #  volumeClaimTemplate:
      #    spec:
      #      storageClassName: gluster
      #      accessModes: ["ReadWriteOnce"]
      #      resources:
      #        requests:
      #          storage: 50Gi
      #      selector: {}

loki-stack:
  enabled: false 

  test_pod:
    enabled: true

  loki:
    enabled: true
    isDefault: true
    url: http://{{(include "loki.serviceName" .)}}:{{ .Values.loki.service.port }}
    readinessProbe:
      httpGet:
        path: /ready
        port: http-metrics
      initialDelaySeconds: 45
    livenessProbe:
      httpGet:
        path: /ready
        port: http-metrics
      initialDelaySeconds: 45
    datasource:
      jsonData: "{}"
      uid: ""

  promtail:
    enabled: true
    config:
      logLevel: info
      serverPort: 3101
      clients:
        - url: http://{{ .Release.Name }}:3100/loki/api/v1/push

  fluent-bit:
    enabled: false

  grafana:
    enabled: false

  prometheus:
    enabled: false
    isDefault: false
    url: http://{{ include "prometheus.fullname" .}}:{{ .Values.prometheus.server.service.servicePort }}{{ .Values.prometheus.server.prefixURL }}
    datasource:
      jsonData: "{}"

  filebeat:
    enabled: false

  logstash:
    enabled: false

  # proxy is currently only used by loki test pod
  # Note: If http_proxy/https_proxy are set, then no_proxy should include the
  # loki service name, so that tests are able to communicate with the loki
  # service.
  proxy:
    http_proxy: ""
    https_proxy: ""
    no_proxy: ""